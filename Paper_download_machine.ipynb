{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install biopython\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hHSH5CuD6XD",
        "outputId": "0daa959c-a6e0-44ce-94ba-31ec584cd3a2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez\n",
        "\n",
        "# Set your email here (NCBI requires it for API access)\n",
        "Entrez.email = \"your_email@example.com\"\n",
        "\n",
        "# Function to fetch metadata including title, abstract, and keywords\n",
        "def fetch_pubmed_metadata(query, retmax=5):\n",
        "    # Search PubMed for articles related to the query\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=retmax)\n",
        "    record = Entrez.read(handle)\n",
        "    pubmed_ids = record['IdList']\n",
        "\n",
        "    # Fetch metadata for each PubMed ID\n",
        "    for pubmed_id in pubmed_ids:\n",
        "        fetch_handle = Entrez.efetch(db=\"pubmed\", id=pubmed_id, rettype=\"medline\", retmode=\"text\")\n",
        "        data = fetch_handle.read()\n",
        "\n",
        "        print(f\"PubMed ID: {pubmed_id}\")\n",
        "        print(data)\n",
        "        print('-' * 80)\n",
        "\n",
        "# Example: Fetch metadata for articles related to 'cancer'\n",
        "fetch_pubmed_metadata(\"cancer\")\n"
      ],
      "metadata": {
        "id": "kxEn5oYbD6TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez, Medline\n",
        "\n",
        "Entrez.email = \"your_email@example.com\"\n",
        "\n",
        "def fetch_and_parse_pubmed(query, retmax=1): # can change no of articles\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=retmax)\n",
        "    record = Entrez.read(handle)\n",
        "    pubmed_ids = record['IdList']\n",
        "\n",
        "    fetch_handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pubmed_ids), rettype=\"medline\", retmode=\"text\")\n",
        "    records = Medline.parse(fetch_handle)\n",
        "\n",
        "    for record in records:\n",
        "        title = record.get(\"TI\", \"No title available\")\n",
        "        abstract = record.get(\"AB\", \"No abstract available\")\n",
        "        keywords = record.get(\"OT\", \"No keywords available\")\n",
        "        authors = record.get(\"AU\", \"No authors available\")\n",
        "\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\"Abstract: {abstract}\")\n",
        "        print(f\"Keywords: {keywords}\")\n",
        "        print(f\"Authors: {', '.join(authors)}\")\n",
        "        print('-' * 80)\n",
        "\n",
        "# Fetch metadata for cancer-related articles\n",
        "fetch_and_parse_pubmed(\"cancer\")\n"
      ],
      "metadata": {
        "id": "QjsJsCmeD6P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez, Medline\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set your email here for NCBI API access\n",
        "Entrez.email = \"your_email@example.com\"\n",
        "\n",
        "def fetch_pubmed_data(query, batch_size=100, max_results=1000, csv_filename=\"pubmed_all_data.csv\"):\n",
        "    \"\"\"\n",
        "    Fetches PubMed metadata in batches and saves it to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - query: The search term for PubMed (e.g., \"cancer\").\n",
        "    - batch_size: Number of articles to fetch per request (max 100).\n",
        "    - max_results: Total number of articles to fetch.\n",
        "    - csv_filename: The filename to save the data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize variables for search\n",
        "    all_records = []\n",
        "    current_start = 0\n",
        "\n",
        "    # Loop through batches\n",
        "    while current_start < max_results:\n",
        "        print(f\"Fetching records {current_start + 1} to {current_start + batch_size}\")\n",
        "\n",
        "        # Fetch data using esearch and efetch\n",
        "        handle = Entrez.esearch(db=\"pubmed\", term=query, retstart=current_start, retmax=batch_size)\n",
        "        record = Entrez.read(handle)\n",
        "        pubmed_ids = record['IdList']\n",
        "\n",
        "        if not pubmed_ids:\n",
        "            print(\"No more records found.\")\n",
        "            break\n",
        "\n",
        "        # Fetch metadata for the current batch of PubMed IDs\n",
        "        fetch_handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pubmed_ids), rettype=\"medline\", retmode=\"text\")\n",
        "        records = Medline.parse(fetch_handle)\n",
        "\n",
        "        # Parse each record and extract metadata\n",
        "        for record in records:\n",
        "            title = record.get(\"TI\", \"No title available\")\n",
        "            abstract = record.get(\"AB\", \"No abstract available\")\n",
        "            keywords = record.get(\"OT\", [])  # Keywords are stored in \"OT\" (Other Terms)\n",
        "            authors = \", \".join(record.get(\"AU\", []))  # Authors are stored in \"AU\"\n",
        "            journal = record.get(\"JT\", \"No journal available\")\n",
        "            year = record.get(\"DP\", \"No year available\")\n",
        "\n",
        "            # Add metadata to the list\n",
        "            all_records.append({\n",
        "                \"Title\": title,\n",
        "                \"Abstract\": abstract,\n",
        "                \"Keywords\": \"; \".join(keywords) if keywords else \"No keywords available\",\n",
        "                \"Authors\": authors,\n",
        "                \"Journal\": journal,\n",
        "                \"Year\": year\n",
        "            })\n",
        "\n",
        "        # Increment the starting position for the next batch\n",
        "        current_start += batch_size\n",
        "\n",
        "        # Save the current batch to CSV\n",
        "        df = pd.DataFrame(all_records)\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        print(f\"Batch saved to {csv_filename}\")\n",
        "\n",
        "        # Optional: Introduce a delay to avoid hitting rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"Finished fetching {len(all_records)} records. Data saved to {csv_filename}.\")\n",
        "\n",
        "# Fetch metadata for a large number of publications related to 'cancer' and save to CSV\n",
        "fetch_pubmed_data(query=\"cancer\", batch_size=100, max_results=1000, csv_filename=\"pubmed_cancer_all_data.csv\")\n"
      ],
      "metadata": {
        "id": "A8VRsEtCFejY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aEgXRvddJrZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez, Medline\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set your email for NCBI API access\n",
        "Entrez.email = \"your_email@example.com\"\n",
        "\n",
        "def fetch_large_pubmed_data(query, batch_size=1000, total_records=1000000, csv_filename=\"pubmed_large_data.csv\"):\n",
        "    \"\"\"\n",
        "    Fetches PubMed metadata in batches and saves it to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - query: The search term for PubMed (e.g., \"cancer\").\n",
        "    - batch_size: Number of articles to fetch per request (max 1000 for PubMed API).\n",
        "    - total_records: Total number of records you want to fetch.\n",
        "    - csv_filename: The filename to save the data.\n",
        "    \"\"\"\n",
        "\n",
        "    all_records = []\n",
        "    current_start = 0\n",
        "\n",
        "    # Loop through batches to fetch data\n",
        "    while current_start < total_records:\n",
        "        print(f\"Fetching records {current_start + 1} to {current_start + batch_size}\")\n",
        "\n",
        "        try:\n",
        "            # Fetch data using esearch to get PubMed IDs\n",
        "            search_handle = Entrez.esearch(db=\"pubmed\", term=query, retstart=current_start, retmax=batch_size)\n",
        "            search_record = Entrez.read(search_handle)\n",
        "            pubmed_ids = search_record['IdList']\n",
        "\n",
        "            if not pubmed_ids:\n",
        "                print(\"No more records found.\")\n",
        "                break\n",
        "\n",
        "            # Fetch metadata for the current batch of PubMed IDs\n",
        "            fetch_handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pubmed_ids), rettype=\"medline\", retmode=\"text\")\n",
        "            records = Medline.parse(fetch_handle)\n",
        "\n",
        "            # Parse each record and extract metadata\n",
        "            for record in records:\n",
        "                title = record.get(\"TI\", \"No title available\")\n",
        "                abstract = record.get(\"AB\", \"No abstract available\")\n",
        "                keywords = record.get(\"OT\", [])  # OT for Other Terms (keywords)\n",
        "                authors = \", \".join(record.get(\"AU\", []))  # AU for Authors\n",
        "                journal = record.get(\"JT\", \"No journal available\")\n",
        "                year = record.get(\"DP\", \"No year available\")\n",
        "\n",
        "                # Add metadata to the list\n",
        "                all_records.append({\n",
        "                    \"Title\": title,\n",
        "                    \"Abstract\": abstract,\n",
        "                    \"Keywords\": \"; \".join(keywords) if keywords else \"No keywords available\",\n",
        "                    \"Authors\": authors,\n",
        "                    \"Journal\": journal,\n",
        "                    \"Year\": year\n",
        "                })\n",
        "\n",
        "            # Increment the starting position for the next batch\n",
        "            current_start += batch_size\n",
        "\n",
        "            # Convert to a DataFrame and save incrementally to CSV\n",
        "            df = pd.DataFrame(all_records)\n",
        "            df.to_csv(csv_filename, mode='a', header=not current_start, index=False)\n",
        "\n",
        "            # Clear memory by resetting the list\n",
        "            all_records.clear()\n",
        "\n",
        "            # Introduce a delay to respect rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            # Optional: Retry logic or continue fetching the next batch\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "\n",
        "    print(f\"Finished fetching {current_start} records. Data saved to {csv_filename}.\")\n",
        "\n",
        "# Fetch over 1 million records related to 'cancer' and save incrementally\n",
        "fetch_large_pubmed_data(query=\"cancer\", batch_size=1000, total_records=1000, csv_filename=\"pubmed_large_cancer_data.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnXDfvngIIkx",
        "outputId": "675cc925-c47f-4d5e-8210-892baaa444ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching records 1 to 1000\n",
            "Finished fetching 1000 records. Data saved to pubmed_large_cancer_data.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez, Medline\n",
        "import pandas as pd\n",
        "\n",
        "# Set your email for PubMed access\n",
        "Entrez.email = \"your_email@example.com\"\n",
        "\n",
        "# Function to fetch and parse PubMed data, and save to CSV\n",
        "def fetch_pubmed_to_csv(query, retmax=5, csv_filename=\"pubmed_data.csv\"):\n",
        "    # Search PubMed for articles related to the query\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=retmax)\n",
        "    record = Entrez.read(handle)\n",
        "    pubmed_ids = record['IdList']\n",
        "\n",
        "    # Fetch metadata for the PubMed IDs\n",
        "    fetch_handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pubmed_ids), rettype=\"medline\", retmode=\"text\")\n",
        "    records = Medline.parse(fetch_handle)\n",
        "\n",
        "    # Create a list to store metadata\n",
        "    article_list = []\n",
        "\n",
        "    # Extract relevant fields from each record and add to the list\n",
        "    for record in records:\n",
        "        title = record.get(\"TI\", \"No title available\")\n",
        "        abstract = record.get(\"AB\", \"No abstract available\")\n",
        "        keywords = record.get(\"OT\", [])  # OT for Other Terms (keywords)\n",
        "        authors = \", \".join(record.get(\"AU\", []))  # AU for Authors\n",
        "\n",
        "        # Append metadata to the list as a dictionary\n",
        "        article_list.append({\n",
        "            \"Title\": title,\n",
        "            \"Abstract\": abstract,\n",
        "            \"Keywords\": \"; \".join(keywords) if keywords else \"No keywords available\",\n",
        "            \"Authors\": authors\n",
        "        })\n",
        "\n",
        "    # Create a Pandas DataFrame from the list of dictionaries\n",
        "    df = pd.DataFrame(article_list)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "# Fetch metadata for cancer-related articles and save to CSV\n",
        "fetch_pubmed_to_csv(\"cancer\", retmax=10, csv_filename=\"pubmed_cancer_data.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xznPNigYD6G2",
        "outputId": "dd12de4e-b9af-4f4f-e7fd-4ec002ca09bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to pubmed_cancer_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scholarly"
      ],
      "metadata": {
        "id": "hWYL7QBNrTEe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Function to fetch metadata (including keywords) from CrossRef using DOI\n",
        "def get_crossref_metadata(doi):\n",
        "    url = f\"https://api.crossref.org/works/{doi}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        title = data['message'].get('title', ['No title'])[0]\n",
        "        authors = ', '.join([author['given'] + ' ' + author['family'] for author in data['message'].get('author', [])])\n",
        "        abstract = data['message'].get('abstract', 'No abstract available')\n",
        "        year = data['message'].get('issued', {}).get('date-parts', [['No year']])[0][0]\n",
        "        venue = data['message'].get('container-title', ['No venue'])[0]\n",
        "        keywords = data['message'].get('subject', 'No keywords available')\n",
        "\n",
        "        # Print extracted metadata\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\"Authors: {authors}\")\n",
        "        print(f\"Abstract: {abstract}\")\n",
        "        print(f\"Year: {year}\")\n",
        "        print(f\"Venue: {venue}\")\n",
        "        print(f\"Keywords: {keywords}\")\n",
        "        print('-' * 80)\n",
        "    else:\n",
        "        print('Failed to retrieve metadata')\n",
        "\n",
        "# Example DOI\n",
        "doi = \"10.1001/jama.2020.1585\"  # Replace with the actual DOI from Google Scholar metadata\n",
        "get_crossref_metadata(doi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-wIXwID_i0d",
        "outputId": "f8ea7e96-8e0b-4c3f-8c67-4ac110a58c0e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Clinical Characteristics of 138 Hospitalized Patients With 2019 Novel Coronavirus–Infected Pneumonia in Wuhan, China\n",
            "Authors: Dawei Wang, Bo Hu, Chang Hu, Fangfang Zhu, Xing Liu, Jing Zhang, Binbin Wang, Hui Xiang, Zhenshun Cheng, Yong Xiong, Yan Zhao, Yirong Li, Xinghuan Wang, Zhiyong Peng\n",
            "Abstract: No abstract available\n",
            "Year: 2020\n",
            "Venue: JAMA\n",
            "Keywords: []\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install Bio"
      ],
      "metadata": {
        "id": "2ys-EaRB_w_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez\n",
        "\n",
        "# Set your email to avoid being blocked by NCBI\n",
        "Entrez.email = \"your_email@example.com\"\n",
        "\n",
        "# Function to fetch abstract and keywords from PubMed\n",
        "def get_pubmed_metadata(title):\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=title, retmax=5)\n",
        "    record = Entrez.read(handle)\n",
        "    handle.close()\n",
        "\n",
        "    if record['IdList']:\n",
        "        pubmed_id = record['IdList'][0]\n",
        "        handle = Entrez.efetch(db=\"pubmed\", id=pubmed_id, rettype=\"xml\", retmode=\"text\")\n",
        "        pubmed_record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        article = pubmed_record['PubmedArticle'][0]\n",
        "        title = article['MedlineCitation']['Article']['ArticleTitle']\n",
        "        abstract = article['MedlineCitation']['Article']['Abstract']['AbstractText'][0]\n",
        "        keywords = article['MedlineCitation']['KeywordList'][0] if 'KeywordList' in article['MedlineCitation'] else 'No keywords available'\n",
        "\n",
        "        # Print extracted metadata\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\"Abstract: {abstract}\")\n",
        "        print(f\"Keywords: {', '.join(keywords)}\")\n",
        "    else:\n",
        "        print(\"No article found for the given title.\")\n",
        "\n",
        "# Example title\n",
        "title = \"Cancer \"\n",
        "get_pubmed_metadata(title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fscvWq5_qo1",
        "outputId": "3e242f3a-1033-48e6-ba7c-98b51efe87ac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Anabolic deficits and divergent unfolded protein response underlie skeletal and cardiac muscle growth impairments in the Yoshida hepatoma tumor model of cancer cachexia.\n",
            "Abstract: Cancer cachexia manifests as whole body wasting, however, the precise mechanisms governing the alterations in skeletal muscle and cardiac anabolism have yet to be fully elucidated. In this study, we explored changes in anabolic processes in both skeletal and cardiac muscles in the Yoshida AH-130 ascites hepatoma model of cancer cachexia. AH-130 tumor-bearing rats experienced significant losses in body weight, skeletal muscle, and heart mass. Skeletal and cardiac muscle loss was associated with decreased ribosomal (r)RNA, and hypophosphorylation of the eukaryotic factor 4E binding protein 1. Endoplasmic reticulum stress was evident by higher activating transcription factor mRNA in skeletal muscle and growth arrest and DNA damage-inducible protein (GADD)34 mRNA in both skeletal and cardiac muscles. Tumors provoked an increase in tissue expression of interferon-γ in the heart, while an increase in interleukin-1β mRNA was apparent in both skeletal and cardiac muscles. We conclude that compromised skeletal muscle and heart mass in the Yoshida AH-130 ascites hepatoma model involves a marked reduction translational capacity and efficiency. Furthermore, our observations suggest that endoplasmic reticulum stress and tissue production of pro-inflammatory factors may play a role in the development of skeletal and cardiac muscle wasting.\n",
            "Keywords: cancer, inflammation, ribosomal RNA, unfolded protein response, wasting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install serpapi"
      ],
      "metadata": {
        "id": "o78LhMB_AkcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import serpapi\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "def fetch_google_scholar_data():\n",
        "    params = {\n",
        "        \"engine\": \"google_scholar\",  # Specify the Google Scholar engine\n",
        "        \"q\": \"cancer\",               # Query term\n",
        "        \"api_key\": \"SsocPBoBLEseYidg5Mao1mSA\"  # Replace with your SerpAPI key\n",
        "    }\n",
        "\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "\n",
        "    # Loop through the results to extract metadata\n",
        "    for result in results.get(\"organic_results\", []):\n",
        "        print(f\"Title: {result['title']}\")\n",
        "        print(f\"Link: {result['link']}\")\n",
        "        print(f\"Snippet: {result['snippet']}\")\n",
        "        print(f\"Full Abstract: {result.get('inline_snippet', 'No abstract available')}\")\n",
        "        keywords = result.get('keywords', 'No keywords available')\n",
        "        print(f\"Keywords: {keywords}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "fetch_google_scholar_data()\n"
      ],
      "metadata": {
        "id": "CwUA0jJEAh2e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install google-search-results\n"
      ],
      "metadata": {
        "id": "w-g4Y58ABIf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "from scholarly import scholarly\n",
        "\n",
        "search_query = scholarly.search_pubs('cancer')\n",
        "\n",
        "for i in range(10):  # Limiting to 10 results\n",
        "    paper = next(search_query)\n",
        "    print(paper['bib']['title'])\n",
        "    time.sleep(15)  # Delay to avoid rate-limiting\n",
        "\n",
        "# from scholarly import scholarly\n",
        "\n",
        "# # Search for the term \"cancer\"\n",
        "# search_query = scholarly.search_pubs('cancer')\n",
        "\n",
        "# # Iterate through the search results and extract metadata\n",
        "# for i in range(5):  # Limiting to the first 5 results for simplicity\n",
        "#     paper = next(search_query)\n",
        "#     print(f\"Title: {paper['bib']['title']}\")\n",
        "#     print(f\"Authors: {', '.join(paper['bib']['author'])}\")\n",
        "#     print(f\"Abstract: {paper['bib'].get('abstract', 'No abstract available')}\")\n",
        "#     print(f\"Year: {paper['bib'].get('pub_year', 'No year available')}\")\n",
        "#     print(f\"Venue: {paper['bib'].get('venue', 'No venue available')}\")\n",
        "#     print(f\"Number of citations: {paper.get('num_citations', 'No citation count available')}\")\n",
        "\n",
        "#     # Checking for keywords\n",
        "#     keywords = paper['bib'].get('keywords', 'No keywords available')\n",
        "#     print(f\"Keywords: {keywords}\")\n",
        "\n",
        "#     print(f\"Google Scholar URL: {paper['pub_url']}\")\n",
        "#     print('-' * 80)\n"
      ],
      "metadata": {
        "id": "i1aFc_4U-6ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Save the data to a CSV file\n",
        "import csv\n",
        "\n",
        "# Create or open a CSV file to store the results\n",
        "with open('cancer_research_metadata.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Title', 'Authors', 'Abstract', 'Year', 'Venue', 'Citations', 'URL'])\n",
        "\n",
        "    search_query = scholarly.search_pubs('cancer')\n",
        "    for i in range(5):  # Limiting to the first 5 results\n",
        "        paper = next(search_query)\n",
        "        title = paper['bib']['title']\n",
        "        authors = ', '.join(paper['bib']['author'])\n",
        "        abstract = paper['bib'].get('abstract', 'No abstract available')\n",
        "        year = paper['bib'].get('pub_year', 'No year available')\n",
        "        venue = paper['bib'].get('venue', 'No venue available')\n",
        "        citations = paper.get('num_citations', 'No citation count available')\n",
        "        url = paper['pub_url']\n",
        "        writer.writerow([title, authors, abstract, year, venue, citations, url])\n",
        "\n",
        "print(\"Metadata extraction complete, and saved to cancer_research_metadata.csv\")\n"
      ],
      "metadata": {
        "id": "mtLfO5R2rS9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzHnKYskrS35"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}